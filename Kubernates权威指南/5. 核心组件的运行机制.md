## API Server
### 概述
通过进程`kube-apiserver`提供服务，运行在Master上，通过8080端口的HTTP或者6443端口HTTPS提供服务。kubectl与其之间的接口是RESTful API。

运行在Pod中的进程也可以调用K8S API，通常用来实现分布式集群搭建的目的。API Server本身也是一个Service，他的ClusterIP也是ClusterIP池中的第一个IP地址。

以下方法保证了API Server的性能：
1. 协程+队列的轻量级高性能并发代码
2. 普通List接口接合异步Watch接口
3. 高性能的etcd数据库

架构层级
1. API层：提供API接口
2. 访问控制层
3. 注册表层：把所有资源对象都保存在注册表中，并定义了对于资源对象的操作
4. etcd数据库：持久化的KV数据库

### 解读List-Watch机制
![](5.%20核心组件的运行机制/Pasted%20image%2020220731201306.png)
`etcd`提供了Watch API接口，API Server可以监听在etcd上面发生的数据操作事件，这些事件发生之后，etcd会及时通知API Server。为了让其他组件在不访问底层数据库的情况下也能及时获取资源对象的变化之间，API Server也模仿etcd实现了自己的Watch API。

K8S实现数据同步的代码逻辑为：
1. 客户端调用API Server的List接口，获取相关资源的全量数据并将其缓存到内存中
2. 启动对相应资源对象的Watch协程，收到Watch事件之后在进行同步修改

### CRD 在 API Server 中的设计和实现
每一种官方内建资源对象都包含以下主要功能：
1. 资源对象的元数据定义：定义了对应资源对象的数据结构，官方内建资源对象的元数据固化在源码
2. 资源对象的校验逻辑：确保用户提交的资源对象属性的合法性
3. 资源对象的CURD操作代码
4. 资源对象的自动控制器

每个CRD都需要实现上面的功能，直接编写YAML定义文件即可实现以上三个功能，最后一个功能由于有大量的API库和易用的List-Watch变成框架，所以自动控制器的编程难度大大降低。

### Proxy API 接口
代理REST请求，把收到的请求转发到某Node上的Kubelet守护进程的REST端口，Kubelet负责响应。

还可以将请求转发到Pod的相应端口，直接访问Pod的服务，当然也可以转发给Service。

## Controller Manage解析
Controller通过API Server的接口实时监控集群中特定资源的状态变化，并调整其状态为期望的状态。而Controller Manage就是这些Controller的管理者。

### Replication Controller 和 Deployment Controller
副本控制器和资源对象Replication Controller是不一样的，这里区分一下，资源对象简称为RC。副本控制器的核心作用是确保集群中某个RC关联的Pod副本数量在任何时候都保持预设值。

RC中的Pod模板就像一个模具，制作出来的东西一旦脱离模具，模具再怎么变化也不会影响到已经创建的Pod了。Pod也可以修改他的标签脱离RC的掌控。

Deployment可以看作RC的替代者。Deployment Controller在工作过程中实际上是在控制两类相关的资源对象：Deployment和ReplicaSet，我们创建Deployment之后，Deployment Controller也会自动创建ReplicaSet，滚动升级也是创建新的ReplicaSet来实现的。

Deployment Controller的作用如下：
1. 确保在当前集群中有且仅有N个Pod实例
2. 通过调整`spec.replicas`属性的值来实现系统扩容或者缩容
3. 通过改变Pod模板来实现系统的滚动升级

### Node Controller
kubelet进程在启动时通过API Server注册自身节点信息，并定时向API Server汇报自身情况，这些信息会更新到etcd中。Node Controller通过API Server实时获取Node的相关信息，实现管理和控制集群中各个Node的相关控制功能。

### ResourceQuota Controller
资源配额管理，可以支持三个层次的资源配额管理
1. 容器级别：CPU和内存
2. Pod级别：一个Pod内所有容器的可用资源
3. Namespace可以限制：Pod数量、RC数量、Service数量、ResourceQuota数量等

### NameSpace Controller
命名空间的优雅删除和期限删除。

### Service Controller 和 Endpoints Controller
Endpoints Controller监听Service和对应的Pod副本的变化，进行自动的增删改查。

Service Controller 是K8S集群和外部云平台之间的一个接口控制器，检测Service的变化，Service类型为LoadBalancer的时候，确保该Service对应的：LoadBalancer实例在外部的云平台上被相应的创建、删除和更新。

## Scheduler 原理解析
Scheduler是负责Pod调度的组件。

### 调度流程
Scheduler负责接收Controller Manager创建的新Pod，并为其安排相应的目标Node。安置工作完成之后Node上的Kubelet接管后续工作，负责Pod剩下的生命周期。

具体来说，就是将待调度的Pod按照特定的调度算法和调度策略绑定到某个合适的Node上，并且将绑定信息写入etcd中。随后kublet监听到绑定信息，会获取对应的的Pod清单，下载镜像并启动容器。

#### 旧的两阶段调度流程
1. 过滤：遍历所有的Node，筛选出符合要求的节点
2. 打分：采用优选策略对候选节点进行打分，选择分数最高的节点

#### Scheduler Framework
新的Scheduler Framework在旧流程的基础上增加了一些扩展点，同时支持用户以插件的方式进行扩展。我们可以用Scheduling Profiles对其进行自定义配置。


## kubelet运行机制解析
每个Node都会启动一个kubelet服务进程，用于处理Master下发到本节点的任务、管理Pod和Pod里面的容器。kubelet进程都会在API Server上注册节点自身的信息，定期向Master汇报节点资源的使用情况，并通过cAdvisor监控容器和节点资源。

### 节点管理
kubelet启动的时候可以设置进行自注册模式，自动通过API Server注册节点信息，并定时发送节点的新消息，API Server会将这些信息写入etcd中

### Pod管理
kubelet通过以下方式获取自身Node上要运行的Pod清单：
1. 静态Pod配置文件：启动参数`--config`指定目录
2. HTTP端点
3. API Server

不是通过API Server创建的Pod都是静态Pod，kubelet会将静态Pod的状态汇报给API Server，Server会创建一个Mirror Pod与其匹配，来反应静态Pod的状态。

kubelet监听etcd，如果有新的Pod绑定到本地Node，则按照要求创建。

### 容器健康检查
两类探针：
1. `LivenessProbe`：容器是否健康，并根据重启策略进行相应处理
2. `ReadinessProbe`：判断容器是否启动完成且准备接受请求

`LivenessProbe`三种实现方式
1. `ExecAction`：运行一个命令，看退出码
2. `TCPSocketAction`：TCP检查
3. `HttpGetAction`

### cAdvisor 资源监控
开源的分析容器资源使用率和性能特性的代理工具。Kubelet通过cAdvisor获取其所在节点及容器上的数据，cAdvisor自动查找所在Node上的所有容器，自动采集资源使用的统计信息。

新的K8S监控体系中，Metrics Server用于提供核心指标，包括Node和Pod的CPU和内存的使用数据，其他自定义指标则由第三方组件采集和存储。

### 容器运行时
Kubelet需要通过进程间的某种调用方式来实现和容器引擎之间的调用控制功能。

低级容器运行时，包括LXC、runC，不涉及镜像操作功能，也没有对外提供远程编程接口以方便其他应用集成。高级容器运行时，如containerd，被设计成嵌入一个更大的系统中使用，底层驱动runC来实现底层的容器运行时，对外提供镜像拉取和基于gRPC接口的容器CRUD封装接口。

所有的高级运行时都实现了K8S提出的CRI接口规范，所以Kubelet就可以通过CRI插件来实现容器的全生命周期控制了。

CRI技术的成熟和Container Runtime的不断涌现，用户需求在一个K8S集群中配置并启用多种容器运行时，不同类型的Pod选择不同的运行时以实现资源占用或者性能、稳定性等方面的优化。K8S因为这一需求引入了RuntimeClass，可以创建RuntimeClass资源并通过Pod的`spec.runtimeClassName`字段进行关联。kubelet会通过CRI接口调用指定的运行时创建Pod。

## kube-Proxy运行机制解析
K8S创建服务时会给其分配一个虚拟IP地址，客户端访问这个虚拟IP地址来访问服务，服务将其请求转发到后段的Pod上。这种反向代理与普通的反向代理有一些不同，他的IP是虚拟的，从外部访问还需要一点技巧。

### 第一代Proxy
K8S在每个Node上都会运行一个kube-proxy服务进程，核心功能是将某个Service的访问请求转发到多个Pod上。起初是一个真实的TCP/UDP代理，称为用户空间代理模式。当客户端以ClusterIP访问某个Service的时候，就被Node所在的iptables转发给kube-proxy进程，然后由其建立起到后端Pod的T CP/UDP链接，再将请求转发到某个后端Pod上，并实现负载均衡。

### 第二代Proxy
第二代不再起到数据层面的代理作用，Client向Service的请求流量通过iptables的NAT机制直接发送给目标Pod，kube-proxy只承担控制层面的功能那个，通过API Server的Watch接口实时跟踪Service和Endpoints的变更信息，并更新Node节点上相应的iptables规则。

由于完全工作在内核态，不再经过用户态的中转，因此性能更强。

### 第三代Proxy
当集群中的Service和Pod大量增加，第二代会使iptables中的规则急速膨胀，导致性能明显下降。因此引入第三代的`IPVS, IP Virtual Server`模式。

虽然和iptables都是基于Netfilter实现的，但是使用了更高效的哈希表数据结构，允许几乎无限的规模扩张，具有以下优势：
1. 更好的可扩展性和性能
2. 更复杂的复制均衡算法
3. 支持服务器健康检查和连接重试等功能
4. 可以动态修改ipset的集合

iptables规则链是线性数据结构，ipset是带索引的数据结构，因此规则很多的时候也可以高效查找和匹配。kube-proxy在一些场景还需要与iptables协作，但是使用了iptables的扩展ipset。大大的减少了服务器iptables规则的数量，减少了性能损耗。
