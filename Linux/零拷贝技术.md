# DMA技术
## 传统的IO过程
1. CPU 发出对应的指令给磁盘控制器，然后返回；
2. 磁盘控制器收到指令后，于是就开始准备数据，会把数据放入到磁盘控制器的内部缓冲区中，然后产生一个中断；
3. CPU 收到中断信号后，停下手头的工作，接着把磁盘控制器的缓冲区的数据一次一个字节地读进自己的寄存器，然后再把寄存器里的数据写入到内存，而在数据传输的期间 CPU 是无法执行其他任务的。

![[Pasted image 20220503194458.png]]

DMA 技术就是，在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务。
1. 用户进程调用 read 方法，向操作系统发出 I/O 请求，请求读取数据到自己的内存缓冲区中，进程进入阻塞状态；
2. 操作系统收到请求后，进一步将 I/O 请求发送 DMA，然后让 CPU 执行其他任务；
3. DMA 进一步将 I/O 请求发送给磁盘；
4. 磁盘收到 DMA 的 I/O 请求，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向 DMA 发起中断信号，告知自己缓冲区已满；
5. DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷贝到内核缓冲区中，此时不占用 CPU，CPU 可以执行其他任务；
6. 当 DMA 读取了足够多的数据，就会发送中断信号给 CPU；
7. CPU 收到 DMA 的信号，知道数据已经准备好，于是将数据从内核拷贝到用户空间，系统调用返回；

![[Pasted image 20220503200009.png]]

# 文件传输过程
文件传输最简单的方式是：将磁盘上的文件读取出来，然后通过网络协议发送给客户端

![[Pasted image 20220503200152.png]]

## 性能损耗
1. 四次用户态与内核态的上下文切换，因为发生了两次系统调用，一次是 `read()` ，一次是 `write()`，每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。
2. 四次数据拷贝
	-   `第一次拷贝`，把磁盘上的数据拷贝到操作系统内核的缓冲区里：`DMA`
	-   `第二次拷贝`，把内核缓冲区的数据拷贝到用户的缓冲区里：`CPU`
	-   `第三次拷贝`，把拷贝到用户缓冲区里的数据，再拷贝到内核的 socket 的缓冲区里：`cpu`
	-   `第四次拷贝`，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里：`DMA`

要想提高文件传输的性能，就需要减少「用户态与内核态的上下文切换」和「内存拷贝」的次数

## 如何实现零拷贝？

零拷贝技术实现的方式通常有 2 种：
-   `mmap` + `write`
-   `sendfile`

### mmap + write

在前面我们知道，`read()` 系统调用的过程中会把内核缓冲区的数据拷贝到用户的缓冲区里，于是为了减少这一步开销，我们可以用 `mmap()` 替换 `read() `系统调用函数。

```java
buf = mmap(file, len);
write(sockfd, buf, len);
```
`mmap()` 系统调用函数会直接把内核缓冲区里的数据「映射」到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作

![[Pasted image 20220503202935.png]]

具体过程如下：
1. 应用进程调用了 `mmap()` 后，`DMA` 会把磁盘的数据拷贝到内核的缓冲区里。接着，应用进程跟操作系统内核「共享」这个缓冲区；
2. 应用进程再调用 `write()`，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 CPU 来搬运数据；
3. 最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里

### sendfile

在 Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用函数 `sendfile()`，函数形式如下：

```cpp
#include <sys/socket.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```
前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度，返回值是实际复制数据的长度。

首先，它可以替代前面的 read() 和 write() 这两个系统调用，这样就可以减少一次系统调用，也就减少了 2 次上下文切换的开销。

其次，该系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态，这样就只有 2 次上下文切换，和 3 次数据拷贝。

![[Pasted image 20220503210057.png]]

如果网卡支持 SG-DMA（**The Scatter-Gather Direct Memory Access**）技术，我们可以进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区的过程。

你可以在你的 Linux 系统通过下面这个命令，查看网卡是否支持 scatter-gather 特性：

```bash
ethtool -k eth0 | grep scatter-gather
scatter-gather: on
```

于是，从 Linux 内核 2.4 版本开始起，对于支持网卡支持 SG-DMA 技术的情况下， `sendfile() `系统调用的过程发生了点变化

![[Pasted image 20220503210253.png]]

具体过程如下：
-   第一步，通过 DMA 将磁盘上的数据拷贝到内核缓冲区里；
-   第二步，缓冲区描述符和数据长度传到 `socket` 缓冲区，这样网卡的 SG-DMA 控制器就可以直接将内核缓存中的数据拷贝到网卡的缓冲区里，此过程不需要将数据从操作系统内核缓冲区拷贝到 socket 缓冲区中，这样就减少了一次数据拷贝

所以，这个过程之中，只进行了 2 次数据拷贝。

这就是所谓的零拷贝（Zero-copy）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。

零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数，只需要 2 次上下文切换和数据拷贝次数，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU，2都是由 DMA 来搬运。所以，总体来看，零拷贝技术可以把文件传输的性能提高至少一倍以上。


### `Linux 2.6.17` 支持splice
数据从磁盘读取到OS内核缓冲区后，在内核缓冲区直接可将其转成内核空间其他数据buffer，而不需要拷贝到用户空间。  从磁盘读取到内核buffer后，在内核空间可以直接与socket buffer建立pipe管道。和`sendfile()`不同的是，`splice()`不需要硬件支持。

注意`splice`和`sendfile`的不同：
- `sendfile`将数据加载到`kernel buffer`后，需要一次`CPU copy`拷贝到`socket buffer` 
- `splice`不需要`CPU copy`，直接将两个内核空间的buffer进行`set up pipe`

`splice`会经历 
- 2次拷贝
- 0次cpu copy 
- 2次DMA copy 
- 2次上下文切换